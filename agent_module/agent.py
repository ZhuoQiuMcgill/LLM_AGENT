"""
Core Agent module that handles the interaction with LLMs.
"""

import logging
from typing import Dict, List, Optional, Any, Union

# Use relative imports for better package structure
from .llm.base import LLMInterface
from .exceptions import AgentError

# Forward reference for type hints
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .history.memory import HistoryManager

logger = logging.getLogger(__name__)


class Agent:
    """
    Core Agent class that manages interactions with a chosen LLM.

    The Agent provides a unified interface for sending prompts to different LLMs,
    while maintaining conversation history and handling the necessary configurations.
    """

    def __init__(
            self,
            llm_interface: LLMInterface,
            history_manager: Optional['HistoryManager'] = None,
            name: str = "Assistant",
            system_prompt: Optional[str] = None,
    ):
        """
        Initialize the Agent.

        Args:
            llm_interface: An implementation of LLMInterface to handle LLM interactions.
            history_manager: A HistoryManager instance to handle conversation history.
            name: A name for the Agent (used mainly for logging and identification).
            system_prompt: An optional system prompt to override the default one in the LLM.
        """
        self.llm = llm_interface

        # Use provided history manager or import and create a default one
        if history_manager is None:
            from .history.memory import InMemoryHistoryManager
            self.history_manager = InMemoryHistoryManager()
        else:
            self.history_manager = history_manager

        self.name = name
        self.system_prompt = system_prompt

        logger.debug(f"Initialized Agent: {name}")

    async def process(self, input_text: str, config: Optional[Dict[str, Any]] = None) -> str:
        """
        Process an input message through the Agent and generate a response.

        This method:
        1. Gets the conversation history from the history manager
        2. Combines the system prompt with the history and current input
        3. Sends this to the LLM for processing
        4. Updates the history with the new input and response
        5. Returns the LLM's response

        Args:
            input_text: The input text/prompt from the user.
            config: Optional configuration overrides for this specific request.

        Returns:
            The text response generated by the LLM.

        Raises:
            AgentError: If there's an error in processing the request.
        """
        try:
            # Get conversation history
            history = self.history_manager.get_history()

            # Prepare configuration
            request_config = self._prepare_request_config(config)

            # Generate response using the LLM
            response = await self.llm.generate_response(
                prompt=input_text,
                history=history,
                config=request_config
            )

            # Update history with the new input and response
            self.history_manager.add_user_message(input_text)
            self.history_manager.add_assistant_message(response)

            logger.debug(f"Agent {self.name} processed input and generated response")

            return response

        except Exception as e:
            logger.error(f"Error processing input with Agent {self.name}: {str(e)}")
            raise AgentError(f"Error processing input: {str(e)}") from e

    async def run(self, input_text: str, config: Optional[Dict[str, Any]] = None) -> str:
        """
        Alias for process() method to provide a more intuitive API.

        Args:
            input_text: The input text/prompt from the user.
            config: Optional configuration overrides for this specific request.

        Returns:
            The text response generated by the LLM.
        """
        return await self.process(input_text, config)

    def _prepare_request_config(self, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Prepare the request configuration.

        Args:
            config: Optional configuration overrides.

        Returns:
            A dictionary with the configuration for the LLM request.
        """
        request_config = {}

        # Add system prompt if available
        if self.system_prompt:
            request_config["system_prompt"] = self.system_prompt

        # Add any additional config parameters
        if config:
            request_config.update(config)

        return request_config

    def reset_history(self) -> None:
        """
        Reset the conversation history.
        """
        self.history_manager.clear()
        logger.debug(f"History reset for Agent {self.name}")
